{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNvITXHg_2PZ"
      },
      "source": [
        "# HuBERT Pre-training Pipeline (Colab Pro)\n",
        "\n",
        "Pipeline completo: clone → datos → chunk index → assign labels → training.\n",
        "\n",
        "Todo queda en Google Drive para persistencia entre sesiones.\n",
        "\n",
        "**Prerequisitos:**\n",
        "- Colab Pro con GPU (A100 o V100)\n",
        "- Token de HuggingFace con acceso al dataset `MLCommons/unsupervised_peoples_speech`\n",
        "- Suficiente espacio en Drive (~50+ GB para tars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4qTanqa_2PZ"
      },
      "source": [
        "## 0. Setup: Drive + Repo + Deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRpE4a___2Pa"
      },
      "outputs": [],
      "source": [
        "# Montar Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_4yh7N8_2Pa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directorio base en Drive — todo persistente aquí\n",
        "DRIVE_BASE = '/content/drive/MyDrive/ups-challenge'\n",
        "DRIVE_DATA = f'{DRIVE_BASE}/data'\n",
        "DRIVE_TAR_CACHE = f'{DRIVE_DATA}/tar_cache'\n",
        "DRIVE_CHECKPOINTS = f'{DRIVE_BASE}/checkpoints'\n",
        "\n",
        "os.makedirs(DRIVE_DATA, exist_ok=True)\n",
        "os.makedirs(DRIVE_TAR_CACHE, exist_ok=True)\n",
        "os.makedirs(DRIVE_CHECKPOINTS, exist_ok=True)\n",
        "\n",
        "print('Drive dirs ready:')\n",
        "!ls -la {DRIVE_BASE}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HWpNeRM_2Pa"
      },
      "outputs": [],
      "source": [
        "# Clonar repo (si no está ya clonado)\n",
        "REPO_DIR = '/content/ups-challenge-baselines'\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone https://github.com/dannersm/ups-challenge-baselines.git {REPO_DIR}\n",
        "else:\n",
        "    !cd {REPO_DIR} && git pull\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "!git log --oneline -5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2Pl1wua_2Pb"
      },
      "outputs": [],
      "source": [
        "# Symlinks: data/ y checkpoints/ apuntan a Drive\n",
        "import os\n",
        "\n",
        "for local, drive_path in [('data', DRIVE_DATA), ('checkpoints', DRIVE_CHECKPOINTS)]:\n",
        "    local_path = os.path.join(REPO_DIR, local)\n",
        "    if os.path.islink(local_path):\n",
        "        os.unlink(local_path)\n",
        "    elif os.path.exists(local_path):\n",
        "        # Si existe un dir real, moverlo a Drive primero\n",
        "        !mv {local_path}/* {drive_path}/ 2>/dev/null; rm -rf {local_path}\n",
        "    os.symlink(drive_path, local_path)\n",
        "\n",
        "print('Symlinks:')\n",
        "!ls -la data checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJeqj9CC_2Pb"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias\n",
        "# Nota: Colab puede tener Python 3.10/3.11, relajamos la restricción\n",
        "!sed -i 's/requires-python = \">=3.12\"/requires-python = \">=3.10\"/' pyproject.toml\n",
        "!pip install -e . 2>&1 | tail -5\n",
        "!pip install webdataset\n",
        "print('\\n--- Verificación ---')\n",
        "!python -c \"import torch; print(f'PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}')\"\n",
        "!python -c \"import torchcodec; print(f'torchcodec OK')\"\n",
        "!python -c \"import webdataset; print(f'webdataset OK')\"\n",
        "!python -c \"import transformers; print(f'transformers {transformers.__version__}')\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0diybMR_2Pb"
      },
      "outputs": [],
      "source": [
        "# HuggingFace token — ponlo como secret de Colab o pégalo aquí\n",
        "import os\n",
        "\n",
        "# Opción 1: Colab Secrets (recomendado)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "    print('HF_TOKEN cargado desde Colab Secrets')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Opción 2: Manual (descomenta y pega)\n",
        "# os.environ['HF_TOKEN'] = 'hf_XXXXX'\n",
        "\n",
        "assert os.environ.get('HF_TOKEN'), 'HF_TOKEN no configurado!'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuvAkZZs_2Pb"
      },
      "outputs": [],
      "source": [
        "# Verificar GPU\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(f'\\nGPU: {torch.cuda.get_device_name(0)}')\n",
        "print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3VBzXq__2Pc"
      },
      "source": [
        "## 1. Descargar archivos base (JSONL)\n",
        "\n",
        "Necesitamos `vad_results.jsonl` y `lang_id_results.jsonl` del dataset de HuggingFace.\n",
        "\n",
        "**Estos archivos son grandes (~varios GB).** Se guardan en Drive para no re-descargar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDzgB2L8_2Pc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "HF_TOKEN = os.environ['HF_TOKEN']\n",
        "BASE_URL = 'https://huggingface.co/datasets/MLCommons/unsupervised_peoples_speech/resolve/main'\n",
        "\n",
        "# Descargar vad_results.jsonl\n",
        "vad_path = 'data/vad_results.jsonl'\n",
        "if not os.path.exists(vad_path):\n",
        "    print('Descargando vad_results.jsonl (puede tomar varios minutos)...')\n",
        "    !curl -L -o {vad_path} -H \"Authorization:Bearer {HF_TOKEN}\" \\\n",
        "        \"{BASE_URL}/vad_results.jsonl\"\n",
        "    !ls -lh {vad_path}\n",
        "else:\n",
        "    print(f'vad_results.jsonl ya existe ({os.path.getsize(vad_path)/1e9:.2f} GB)')\n",
        "\n",
        "# lang_id_results.jsonl se descarga automáticamente en el paso de build_lid_index\n",
        "lid_path = 'data/lang_id_results.jsonl'\n",
        "if not os.path.exists(lid_path):\n",
        "    print('Descargando lang_id_results.jsonl...')\n",
        "    !curl -L -o {lid_path} -H \"Authorization:Bearer {HF_TOKEN}\" \\\n",
        "        \"{BASE_URL}/lang_id_results.jsonl\"\n",
        "    !ls -lh {lid_path}\n",
        "else:\n",
        "    print(f'lang_id_results.jsonl ya existe ({os.path.getsize(lid_path)/1e6:.1f} MB)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIus3AaA_2Pc"
      },
      "source": [
        "## 2. Construir VAD shards\n",
        "\n",
        "Parsea `vad_results.jsonl` → `data/vad_shards/{NNNNNN}.pkl` (uno por tar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiWRWbkM_2Pc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Verificar si ya hay shards\n",
        "shard_dir = 'data/vad_shards'\n",
        "if os.path.exists(shard_dir) and len(os.listdir(shard_dir)) > 50:\n",
        "    print(f'VAD shards ya existen: {len(os.listdir(shard_dir))} archivos')\n",
        "    print('Saltando. Si quieres regenerar, borra data/vad_shards/')\n",
        "else:\n",
        "    !python -m ups_challenge.vad_analysis.vad_lookup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFwuabNM_2Pc"
      },
      "source": [
        "## 3. Construir VAD density index\n",
        "\n",
        "Calcula la densidad de speech por tar → `data/vad_density_index.pkl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm7x9YwS_2Pc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists('data/vad_density_index.pkl'):\n",
        "    print('vad_density_index.pkl ya existe, saltando.')\n",
        "else:\n",
        "    !python -m ups_challenge.examples.build_vad_density_index \\\n",
        "        --vad_base_dir ./data/vad_shards \\\n",
        "        --output ./data/vad_density_index.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQvq6tqk_2Pc"
      },
      "source": [
        "## 4. Construir LID index\n",
        "\n",
        "Parsea `lang_id_results.jsonl` → `data/lid_index.pkl` + train/test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znUgYqs-_2Pc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists('data/lid_index.pkl'):\n",
        "    print('lid_index.pkl ya existe, saltando.')\n",
        "else:\n",
        "    from ups_challenge.dataloaders.build_index import build_lid_index\n",
        "    build_lid_index('./data/lid_index.pkl', hf_token=os.environ['HF_TOKEN'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppZ-zHt3_2Pc"
      },
      "source": [
        "## 5. Construir chunk index (Phase 1)\n",
        "\n",
        "Selecciona ~100h de chunks de 10s con alta densidad de speech, priorizando idiomas escasos.\n",
        "\n",
        "Output: `data/chunk_index_100h.pkl`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRtzUCx1_2Pc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "TOTAL_HOURS = 100  # Ajusta según necesidad\n",
        "\n",
        "chunk_index_path = f'data/chunk_index_{TOTAL_HOURS}h.pkl'\n",
        "if os.path.exists(chunk_index_path):\n",
        "    import pickle\n",
        "    with open(chunk_index_path, 'rb') as f:\n",
        "        idx = pickle.load(f)\n",
        "    print(f'chunk_index ya existe: {len(idx):,} entries ({len(idx)*10/3600:.1f}h)')\n",
        "    print('Saltando. Si quieres regenerar, borra el archivo.')\n",
        "else:\n",
        "    !python -m ups_challenge.examples.build_chunk_index \\\n",
        "        --total_hours {TOTAL_HOURS} \\\n",
        "        --min_vad_ratio 0.5 \\\n",
        "        --min_chunk_density 0.8 \\\n",
        "        --vad_base_dir ./data/vad_shards \\\n",
        "        --vad_density_index ./data/vad_density_index.pkl \\\n",
        "        --lid_index_path ./data/lid_index.pkl \\\n",
        "        --lang_hours_path ./ups_challenge/examples/lang_speech_hours.json \\\n",
        "        --output ./data/chunk_index_{TOTAL_HOURS}h.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt1a3Vun_2Pc"
      },
      "source": [
        "## 6. Assign labels (Phase 2)\n",
        "\n",
        "Descarga tars necesarios, extrae MFCCs, fit k-means (incremental partial_fit), asigna labels.\n",
        "\n",
        "**Este paso es resumable:** si se cae, al re-ejecutar retoma desde el último checkpoint.\n",
        "\n",
        "Los tars quedan cacheados en `data/tar_cache/` para re-uso en training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuQgcur4_2Pc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Verificar si ya existe un pretraining index\n",
        "existing = glob.glob('data/pretraining_index_*.pkl')\n",
        "if existing:\n",
        "    print(f'Pretraining index ya existe: {existing}')\n",
        "    print('Saltando assign_labels. Si quieres regenerar, borra los archivos.')\n",
        "else:\n",
        "    # Resumable: si se cae, al re-ejecutar retoma desde el checkpoint\n",
        "    !python -m ups_challenge.examples.assign_labels \\\n",
        "        --index ./data/chunk_index_{TOTAL_HOURS}h.pkl \\\n",
        "        --n_clusters 100 \\\n",
        "        --output_dir ./data \\\n",
        "        --cache_dir ./data/tar_cache \\\n",
        "        --target_sr 16000 \\\n",
        "        --save_every_tars 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY4lnork_2Pd"
      },
      "outputs": [],
      "source": [
        "# Verificar el índice generado\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "idx_files = sorted(glob.glob('data/pretraining_index_*.pkl'))\n",
        "for f in idx_files:\n",
        "    with open(f, 'rb') as fh:\n",
        "        idx = pickle.load(fh)\n",
        "    hours = len(idx) * 10 / 3600\n",
        "    langs = len(set(e.get('language', '?') for e in idx))\n",
        "    tars = len(set(e['tar_number'] for e in idx))\n",
        "    print(f'{f}: {len(idx):,} entries, {hours:.1f}h, {langs} langs, {tars} tars')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iVicq85_2Pd"
      },
      "source": [
        "## 7. HuBERT Pre-training (4 epochs)\n",
        "\n",
        "**Resumable:** guarda `training_state.pt` cada 500 optimizer steps. Si se cae, al re-ejecutar retoma automáticamente con `--resume`.\n",
        "\n",
        "**Guía de batch_size por GPU:**\n",
        "- T4 (16GB): `batch_size=8`, `grad_accum=4` → effective 32\n",
        "- V100 (16GB): `batch_size=12-16`, `grad_accum=2-4` → effective 32-64\n",
        "- A100 (40GB): `batch_size=32-48`, `grad_accum=1-2` → effective 32-96\n",
        "- A100 (80GB): `batch_size=64`, `grad_accum=1` → effective 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj8tG08S_2Pd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import glob\n",
        "\n",
        "# Auto-detectar batch size según VRAM\n",
        "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "\n",
        "if vram_gb >= 70:      # A100-80GB\n",
        "    BATCH_SIZE = 64\n",
        "    GRAD_ACCUM = 1\n",
        "elif vram_gb >= 35:    # A100-40GB\n",
        "    BATCH_SIZE = 32\n",
        "    GRAD_ACCUM = 1\n",
        "elif vram_gb >= 14:    # V100/T4\n",
        "    BATCH_SIZE = 12\n",
        "    GRAD_ACCUM = 3\n",
        "else:\n",
        "    BATCH_SIZE = 8\n",
        "    GRAD_ACCUM = 4\n",
        "\n",
        "EFFECTIVE_BATCH = BATCH_SIZE * GRAD_ACCUM\n",
        "print(f'GPU: {gpu_name} ({vram_gb:.0f} GB VRAM)')\n",
        "print(f'batch_size={BATCH_SIZE}, grad_accum={GRAD_ACCUM}, effective={EFFECTIVE_BATCH}')\n",
        "\n",
        "# Encontrar el index más reciente\n",
        "INDEX_PATH = sorted(glob.glob('data/pretraining_index_*.pkl'))[-1]\n",
        "INDEX_PATH = 'data/pretraining_index_97h.pkl'\n",
        "print(f'Index: {INDEX_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggZhue1p_2Pd"
      },
      "outputs": [],
      "source": [
        "# Entrenar (resumable — si se cae, re-ejecuta esta celda)\n",
        "!python -m ups_challenge.examples.hubert_pretraining \\\n",
        "    --index_path {INDEX_PATH} \\\n",
        "    --num_clusters 100 \\\n",
        "    --batch_size {BATCH_SIZE} \\\n",
        "    --grad_accum_steps {GRAD_ACCUM} \\\n",
        "    --num_epochs 4 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --warmup_steps 500 \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --cache_dir ./data/tar_cache \\\n",
        "    --output_dir ./checkpoints/aligned \\\n",
        "    --save_every_steps 500 \\\n",
        "    --projection_warmup_epochs 1 \\\n",
        "    --projection_lr 5e-4 #\\\n",
        "    #--resume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKxn_fdI_2Pd"
      },
      "outputs": [],
      "source": [
        "# Verificar checkpoints guardados en Drive\n",
        "!ls -lh checkpoints/aligned/\n",
        "\n",
        "# Mostrar loss curve si existe\n",
        "import os\n",
        "if os.path.exists('checkpoints/aligned/loss_curve.png'):\n",
        "    from IPython.display import Image, display\n",
        "    display(Image('checkpoints/aligned/loss_curve.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuuaS1CW_2Pd"
      },
      "source": [
        "## 8. Espacio en Drive\n",
        "\n",
        "Utilidades para monitorear y limpiar espacio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19Q7GKa1_2Pd"
      },
      "outputs": [],
      "source": [
        "# Espacio usado por componente\n",
        "!echo '--- tar_cache ---' && du -sh data/tar_cache/ 2>/dev/null\n",
        "!echo '--- vad_shards ---' && du -sh data/vad_shards/ 2>/dev/null\n",
        "!echo '--- indices/pkl ---' && du -sh data/*.pkl 2>/dev/null\n",
        "!echo '--- checkpoints ---' && du -sh checkpoints/ 2>/dev/null\n",
        "!echo '--- JSONL ---' && ls -lh data/*.jsonl 2>/dev/null\n",
        "!echo '--- TOTAL ---' && du -sh data/ checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ajIkA1d_2Pd"
      },
      "outputs": [],
      "source": [
        "# OPCIONAL: Borrar vad_results.jsonl después de generar los shards (ahorra ~GB)\n",
        "# import os\n",
        "# if os.path.exists('data/vad_shards') and len(os.listdir('data/vad_shards')) > 50:\n",
        "#     os.remove('data/vad_results.jsonl')\n",
        "#     print('vad_results.jsonl borrado (shards ya generados)')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}