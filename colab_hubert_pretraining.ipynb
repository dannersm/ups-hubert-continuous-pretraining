{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNvITXHg_2PZ"
   },
   "source": "# HuBERT Pre-training Pipeline (Colab Pro)\n\nFull pipeline: clone → data → chunk index → assign labels → training.\n\nEverything is stored on Google Drive for persistence across sessions.\n\n**Prerequisites:**\n- Colab Pro with GPU (A100 or V100)\n- HuggingFace token with access to the `MLCommons/unsupervised_peoples_speech` dataset\n- Enough Drive space (~50+ GB for tars)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4qTanqa_2PZ"
   },
   "source": [
    "## 0. Setup: Drive + Repo + Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRpE4a___2Pa"
   },
   "outputs": [],
   "source": "# Mount Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_4yh7N8_2Pa"
   },
   "outputs": [],
   "source": "import os\n\n# Base directory on Drive — everything persists here\nDRIVE_BASE = '/content/drive/MyDrive/ups-challenge'\nDRIVE_DATA = f'{DRIVE_BASE}/data'\nDRIVE_TAR_CACHE = f'{DRIVE_DATA}/tar_cache'\nDRIVE_CHECKPOINTS = f'{DRIVE_BASE}/checkpoints'\n\nos.makedirs(DRIVE_DATA, exist_ok=True)\nos.makedirs(DRIVE_TAR_CACHE, exist_ok=True)\nos.makedirs(DRIVE_CHECKPOINTS, exist_ok=True)\n\nprint('Drive dirs ready:')\n!ls -la {DRIVE_BASE}/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HWpNeRM_2Pa"
   },
   "outputs": [],
   "source": "# Clone repo (if not already cloned)\nREPO_DIR = '/content/ups-hubert-continuous-pretraining'\n\nif not os.path.exists(REPO_DIR):\n    !git clone https://github.com/dannersm/ups-hubert-continuous-pretraining.git {REPO_DIR}\nelse:\n    !cd {REPO_DIR} && git pull\n\nos.chdir(REPO_DIR)\n!git log --oneline -5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2Pl1wua_2Pb"
   },
   "outputs": [],
   "source": "# Symlinks: data/ and checkpoints/ point to Drive\nimport os\n\nfor local, drive_path in [('data', DRIVE_DATA), ('checkpoints', DRIVE_CHECKPOINTS)]:\n    local_path = os.path.join(REPO_DIR, local)\n    if os.path.islink(local_path):\n        os.unlink(local_path)\n    elif os.path.exists(local_path):\n        # If a real dir exists, move it to Drive first\n        !mv {local_path}/* {drive_path}/ 2>/dev/null; rm -rf {local_path}\n    os.symlink(drive_path, local_path)\n\nprint('Symlinks:')\n!ls -la data checkpoints"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJeqj9CC_2Pb"
   },
   "outputs": [],
   "source": "# Install dependencies\n# Note: Colab may have Python 3.10/3.11, so we relax the version requirement\n!sed -i 's/requires-python = \">=3.12\"/requires-python = \">=3.10\"/' pyproject.toml\n!pip install -e . 2>&1 | tail -5\n!pip install webdataset\nprint('\\n--- Verification ---')\n!python -c \"import torch; print(f'PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}')\"\n!python -c \"import torchcodec; print(f'torchcodec OK')\"\n!python -c \"import webdataset; print(f'webdataset OK')\"\n!python -c \"import transformers; print(f'transformers {transformers.__version__}')\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0diybMR_2Pb"
   },
   "outputs": [],
   "source": "# HuggingFace token — set it as a Colab secret or paste it here\nimport os\n\n# Option 1: Colab Secrets (recommended)\ntry:\n    from google.colab import userdata\n    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n    print('HF_TOKEN loaded from Colab Secrets')\nexcept Exception:\n    pass\n\n# Option 2: Manual (uncomment and paste)\n# os.environ['HF_TOKEN'] = 'hf_XXXXX'\n\nassert os.environ.get('HF_TOKEN'), 'HF_TOKEN not configured!'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuvAkZZs_2Pb"
   },
   "outputs": [],
   "source": "# Check GPU\n!nvidia-smi\nimport torch\nprint(f'\\nGPU: {torch.cuda.get_device_name(0)}')\nprint(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3VBzXq__2Pc"
   },
   "source": "## 1. Download base files (JSONL)\n\nWe need `vad_results.jsonl` and `lang_id_results.jsonl` from the HuggingFace dataset.\n\n**These files are large (~several GB).** They are saved on Drive to avoid re-downloading."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDzgB2L8_2Pc"
   },
   "outputs": [],
   "source": "import os\n\nHF_TOKEN = os.environ['HF_TOKEN']\nBASE_URL = 'https://huggingface.co/datasets/MLCommons/unsupervised_peoples_speech/resolve/main'\n\n# Download vad_results.jsonl\nvad_path = 'data/vad_results.jsonl'\nif not os.path.exists(vad_path):\n    print('Downloading vad_results.jsonl (may take several minutes)...')\n    !curl -L -o {vad_path} -H \"Authorization:Bearer {HF_TOKEN}\" \\\n        \"{BASE_URL}/vad_results.jsonl\"\n    !ls -lh {vad_path}\nelse:\n    print(f'vad_results.jsonl already exists ({os.path.getsize(vad_path)/1e9:.2f} GB)')\n\n# lang_id_results.jsonl is downloaded automatically in the build_lid_index step\nlid_path = 'data/lang_id_results.jsonl'\nif not os.path.exists(lid_path):\n    print('Downloading lang_id_results.jsonl...')\n    !curl -L -o {lid_path} -H \"Authorization:Bearer {HF_TOKEN}\" \\\n        \"{BASE_URL}/lang_id_results.jsonl\"\n    !ls -lh {lid_path}\nelse:\n    print(f'lang_id_results.jsonl already exists ({os.path.getsize(lid_path)/1e6:.1f} MB)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIus3AaA_2Pc"
   },
   "source": "## 2. Build VAD shards\n\nParses `vad_results.jsonl` → `data/vad_shards/{NNNNNN}.pkl` (one per tar)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiWRWbkM_2Pc"
   },
   "outputs": [],
   "source": "import os\n\n# Check if shards already exist\nshard_dir = 'data/vad_shards'\nif os.path.exists(shard_dir) and len(os.listdir(shard_dir)) > 50:\n    print(f'VAD shards already exist: {len(os.listdir(shard_dir))} files')\n    print('Skipping. To regenerate, delete data/vad_shards/')\nelse:\n    !python -m ups_challenge.preprocessing.vad_lookup"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFwuabNM_2Pc"
   },
   "source": "## 3. Build VAD density index\n\nComputes speech density per tar → `data/vad_density_index.pkl`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sm7x9YwS_2Pc"
   },
   "outputs": [],
   "source": "import os\n\nif os.path.exists('data/vad_density_index.pkl'):\n    print('vad_density_index.pkl already exists, skipping.')\nelse:\n    !python -m ups_challenge.preprocessing.build_vad_density_index \\\n        --vad_base_dir ./data/vad_shards \\\n        --output ./data/vad_density_index.pkl"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQvq6tqk_2Pc"
   },
   "source": "## 4. Build LID index\n\nParses `lang_id_results.jsonl` → `data/lid_index.pkl` + train/test splits."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znUgYqs-_2Pc"
   },
   "outputs": [],
   "source": "import os\n\nif os.path.exists('data/lid_index.pkl'):\n    print('lid_index.pkl already exists, skipping.')\nelse:\n    from ups_challenge.preprocessing.build_lang_index import build_lid_index\n    build_lid_index('./data/lid_index.pkl', hf_token=os.environ['HF_TOKEN'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppZ-zHt3_2Pc"
   },
   "source": "## 5. Build chunk index (Phase 1)\n\nSelects ~100h of 10-second chunks with high speech density, prioritizing scarce languages.\n\nOutput: `data/chunk_index_100h.pkl`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRtzUCx1_2Pc"
   },
   "outputs": [],
   "source": "import os\n\nTOTAL_HOURS = 100  # Adjust as needed\n\nchunk_index_path = f'data/chunk_index_{TOTAL_HOURS}h.pkl'\nif os.path.exists(chunk_index_path):\n    import pickle\n    with open(chunk_index_path, 'rb') as f:\n        idx = pickle.load(f)\n    print(f'chunk_index already exists: {len(idx):,} entries ({len(idx)*10/3600:.1f}h)')\n    print('Skipping. To regenerate, delete the file.')\nelse:\n    !python -m ups_challenge.preprocessing.build_chunk_index \\\n        --total_hours {TOTAL_HOURS} \\\n        --min_vad_ratio 0.5 \\\n        --min_chunk_density 0.8 \\\n        --vad_base_dir ./data/vad_shards \\\n        --vad_density_index ./data/vad_density_index.pkl \\\n        --lid_index_path ./data/lid_index.pkl \\\n        --lang_hours_path ./ups_challenge/preprocessing/lang_speech_hours.json \\\n        --output ./data/chunk_index_{TOTAL_HOURS}h.pkl"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt1a3Vun_2Pc"
   },
   "source": "## 6. Assign labels (Phase 2)\n\nDownloads necessary tars, extracts MFCCs, fits k-means (incremental partial_fit), and assigns labels.\n\n**This step is resumable:** if it crashes, re-running resumes from the last checkpoint.\n\nThe tars are cached in `data/tar_cache/` for reuse during training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuQgcur4_2Pc"
   },
   "outputs": [],
   "source": "import os\nimport glob\n\n# Check if a pretraining index already exists\nexisting = glob.glob('data/pretraining_index_*.pkl')\nif existing:\n    print(f'Pretraining index already exists: {existing}')\n    print('Skipping assign_labels. To regenerate, delete the files.')\nelse:\n    # Resumable: if it crashes, re-running resumes from the checkpoint\n    !python -m ups_challenge.inference.assign_labels \\\n        --index ./data/chunk_index_{TOTAL_HOURS}h.pkl \\\n        --n_clusters 100 \\\n        --output_dir ./data \\\n        --cache_dir ./data/tar_cache \\\n        --target_sr 16000 \\\n        --save_every_tars 5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GY4lnork_2Pd"
   },
   "outputs": [],
   "source": "# Verify the generated index\nimport pickle\nimport glob\n\nidx_files = sorted(glob.glob('data/pretraining_index_*.pkl'))\nfor f in idx_files:\n    with open(f, 'rb') as fh:\n        idx = pickle.load(fh)\n    hours = len(idx) * 10 / 3600\n    langs = len(set(e.get('language', '?') for e in idx))\n    tars = len(set(e['tar_number'] for e in idx))\n    print(f'{f}: {len(idx):,} entries, {hours:.1f}h, {langs} langs, {tars} tars')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iVicq85_2Pd"
   },
   "source": "## 7. HuBERT Pre-training (4 epochs)\n\n**Resumable at epoch level:** saves `training_state.pt` at the end of each epoch. If Colab crashes, re-run with `--resume` to continue from the last completed epoch.\n\n**Batch size guide by GPU:**\n- T4 (16GB): `batch_size=8`, `grad_accum=4` → effective 32\n- V100 (16GB): `batch_size=12-16`, `grad_accum=2-4` → effective 32-64\n- A100 (40GB): `batch_size=32-48`, `grad_accum=1-2` → effective 32-96\n- A100 (80GB): `batch_size=64`, `grad_accum=1` → effective 64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oj8tG08S_2Pd"
   },
   "outputs": [],
   "source": "import torch\nimport glob\n\n# Auto-detect batch size based on VRAM\nvram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\ngpu_name = torch.cuda.get_device_name(0)\n\nif vram_gb >= 70:      # A100-80GB\n    BATCH_SIZE = 64\n    GRAD_ACCUM = 1\nelif vram_gb >= 35:    # A100-40GB\n    BATCH_SIZE = 32\n    GRAD_ACCUM = 1\nelif vram_gb >= 14:    # V100/T4\n    BATCH_SIZE = 12\n    GRAD_ACCUM = 3\nelse:\n    BATCH_SIZE = 8\n    GRAD_ACCUM = 4\n\nEFFECTIVE_BATCH = BATCH_SIZE * GRAD_ACCUM\nprint(f'GPU: {gpu_name} ({vram_gb:.0f} GB VRAM)')\nprint(f'batch_size={BATCH_SIZE}, grad_accum={GRAD_ACCUM}, effective={EFFECTIVE_BATCH}')\n\n# Find the most recent index\nINDEX_PATH = sorted(glob.glob('data/pretraining_index_*.pkl'))[-1]\nINDEX_PATH = 'data/pretraining_index_485h.pkl'\nprint(f'Index: {INDEX_PATH}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggZhue1p_2Pd"
   },
   "outputs": [],
   "source": "# Train (resumable — if it crashes, re-run this cell with --resume)\n!python -m ups_challenge.inference.hubert_pretraining \\\n    --index_path {INDEX_PATH} \\\n    --num_clusters 100 \\\n    --batch_size {BATCH_SIZE} \\\n    --grad_accum_steps {GRAD_ACCUM} \\\n    --num_epochs 4 \\\n    --learning_rate 5e-5 \\\n    --warmup_steps 500 \\\n    --max_grad_norm 1.0 \\\n    --cache_dir ./data/tar_cache \\\n    --output_dir ./checkpoints/aligned \\\n    --projection_warmup_epochs 1 \\\n    --projection_lr 5e-4 \\\n    --resume"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKxn_fdI_2Pd"
   },
   "outputs": [],
   "source": "# Check checkpoints saved on Drive\n!ls -lh checkpoints/aligned/\n\n# Show loss curve if it exists\nimport os\nif os.path.exists('checkpoints/aligned/loss_curve.png'):\n    from IPython.display import Image, display\n    display(Image('checkpoints/aligned/loss_curve.png'))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuuaS1CW_2Pd"
   },
   "source": "## 8. Drive space\n\nUtilities for monitoring and cleaning up space."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19Q7GKa1_2Pd"
   },
   "outputs": [],
   "source": "# Space used by component\n!echo '--- tar_cache ---' && du -sh data/tar_cache/ 2>/dev/null\n!echo '--- vad_shards ---' && du -sh data/vad_shards/ 2>/dev/null\n!echo '--- indices/pkl ---' && du -sh data/*.pkl 2>/dev/null\n!echo '--- checkpoints ---' && du -sh checkpoints/ 2>/dev/null\n!echo '--- JSONL ---' && ls -lh data/*.jsonl 2>/dev/null\n!echo '--- TOTAL ---' && du -sh data/ checkpoints/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ajIkA1d_2Pd"
   },
   "outputs": [],
   "source": "# OPTIONAL: Delete vad_results.jsonl after generating the shards (saves ~GB)\n# import os\n# if os.path.exists('data/vad_shards') and len(os.listdir('data/vad_shards')) > 50:\n#     os.remove('data/vad_results.jsonl')\n#     print('vad_results.jsonl deleted (shards already generated)')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
